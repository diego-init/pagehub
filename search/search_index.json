{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to my page","text":"Diego Da Silva, Ph.D<pre><code>import mini_bio\n\ncurrent_status =  ['Postdoctoral Fellow @ University of Toronto']\n\ntitle = ['Ph.D. in Computer Science - Federal University of ABC (Brazil)']\n\n# Fun Fact: The ABC region spans across three cities in the Sao Paulo Metropolitan Area:\n# Santo Andre, Sao Bernardo do Campo and Sao Caetano do Sul.\n# The initial of each name forms the acronym ABC.\n\ndef bio():\n\n    '''My research connects transit and computational methods to create fair, \n    reliable, and efficient urban mobility. Additionally, I am a Brazilian Jiu-Jitsu \n    black belt and former UofT Grappling Club instructor. \n    I take paper submission seriously.'''\n\nresearch_interests =  ['Learning Methods, Transit Data, Modeling, Simulation']\n</code></pre> <p>Download my CV</p>"},{"location":"#academic-contributions","title":"Academic Contributions","text":""},{"location":"#peer-reviewed-journal-publications","title":"Peer-Reviewed Journal Publications","text":"<p>Da Silva, D., Klumpenhouwer, W., Karner, A., Robinson, M., Liu, R., and Shalaby, A. (2022). Living on a Fare: Modeling and Quantifying the Effects of Fare Budgets on Accessibility and Equity. Journal of Transport Geography 101, p. 103348.</p> <p>Klumpenhouwer, W., Allen, J., Li, L., Liu, R., Robinson, M., Da Silva, D., Farber, S., Karner, A., Rowangould, D., Shalaby, A., Buchanan, M., and Higashide, S. (2021). A Comprehensive Transit Accessibility and Equity Dashboard. Findings (July). https://dashboard.transitcenter.org/</p> <p>Chen, R., Shalaby, A., and  Da Silva, D., (2024). Trends in Toronto\u2019s Subway Ridership Recovery: An Exploratory Analysis of Wi-Fi Records. Transportation Research Record, 0(0). https://doi.org/10.1177/03611981241242775</p> <p>Da Silva, D., Shalaby, A. Forecasting Short-Term Subway Passenger Flow Using Wi-Fi Data Comparative Analysis of Advanced Time-Series Methods. Journal of Intelligent Transportation Systems (Under Review).</p> <p>Othman, K., Ahmed, S., Da Silva, D., Shalaby, A., Abdulhai, B. (2024). Decision Support Tools for Effective Bus Fleet Electrification: Replacement Factors and Fleet Sizing Prediction. Transportation Research Interdisciplinary Perspectives.</p> <p>Othman, K., Da Silva, D., Shalaby, A., Abdulhai, B. (2024). Interpretable Machine Learning Models for Predicting Ebus Battery Consumption Rates in Cold Climates with and without Auxiliary Heating. Green Energy and Intelligent Transportation.</p>"},{"location":"#peer-reviewed-journal-publications-under-review","title":"Peer-Reviewed Journal Publications Under Review","text":"<p>Da Silva, D., Elsaid, F., Shalaby, A. Constructing Origin-Destination Matrix using Wi-Fi and AFC Data. Transportation (Under Review).</p>"},{"location":"#peer-reviewed-conference-publications","title":"Peer-Reviewed Conference Publications","text":"<p>Chen, R., Shalaby, A., Da Silva, D. (2024). Trends in Toronto\u2019s Subway Ridership Recovery: An Exploratory Analysis of Wi-Fi Records. In Proceedings of the 103rd Transportation Research Board. Washington, DC. 32 pages.</p>"},{"location":"#peer-reviewed-conference-presentations","title":"Peer-Reviewed Conference Presentations","text":"<p>Da Silva, D., Shalaby, A. (2024). Forecasting Short-Term Subway Passenger Flow Using Wi-Fi Data Comparative Analysis of Advanced Time-Series Methods. 103rd Annual Meeting of the Transportation Research Board. Washington, DC.</p> <p>Othman, K.,Da Silva, D., Shalaby, A., Abdulhai, B. (2024). Data-Driven Prediction of e-Bus Battery Consumption Rates Using Machine Learning Techniques in the Canadian Environment. 103rd Annual Meeting of the Transportation Research Board. Washington, DC.</p> <p>R Chen, A Shalaby, Da Silva, D. (2024). Trends in Toronto\u2019s Subway Ridership Recovery: An Exploratory Analysis of Wi-Fi Records. 103rd Annual Meeting of the Transportation Research Board. Washington, DC.</p> <p>Da Silva, D., Elsaid, F., Shalaby, A. (2023). Constructing Origin-Destination Demand Matrix Using Wi-Fi and Automated Fare Collection Gate Count Data: A Case Study of Toronto\u2019s Subway Network. 102nd Annual Meeting of the Transportation Research Board. Washington, DC.</p> <p>Da Silva, D., Robinson, M. Liu, R., Klumpenhouwer, W., Shalaby, A., and Karner, A. (2022). A Flexible Itinerary-Based Fare Calculator with Detailed Transfer Modeling. 101st Annual Meeting of the Transportation Research Board. Washington, DC.</p>"},{"location":"#conference-abstracts","title":"Conference Abstracts","text":"<p>Klumpenhouwer, W., Allen, J., Li, L., Liu, R., Robinson, M., Da Silva, D., Farber, S., Karner, A., Rowangould, D., Shalaby, A., Buchanan, M., and Higashide, S. (2021). Dashboarding Transit Accessibility Canadian Urban Transit Association 2021 Virtual Conference. Toronto, Canada.</p> <p>Da Silva, D., Elsaid, F., Shalaby, A. (2023). Constructing Origin-Destination Demand Matrix Using Wi-Fi and Automated Fare Collection Gate Count Data. 25th Ontario Transportation Expo - Conference and Trade Show. Toronto, Canada.</p> <p>Othman, K., Da Silva, D., Shalaby, A., Abdulhai, B. (2024). Data-Driven Prediction of e-Bus Battery Consumption Rates. 25th Ontario Transportation Expo - Conference and Trade Show. Toronto, Canada.</p>"},{"location":"Courses/","title":"Courses","text":""},{"location":"Courses/#civ1599-civ1599-s-analytics-for-transit-and-mobility-systems","title":"CIV1599 - CIV1599 S: Analytics for Transit and Mobility Systems","text":"<p>University of Toronto Summer 2024</p> <p>I assisted Prof. Amer Shalaby in developing and teaching a unique graduate-level course that focuses on the most advanced techniques used in public transit. This course covers topics such as AI models and advanced time series analysis. It emphasizes principles of data analysis, modeling, and visualization, with applications in transit reliability, equity, access to opportunities, demand forecasting, and real-time data analysis.</p>"},{"location":"Courses/#course-description","title":"Course Description","text":"<p>Transit agencies around the world witness a growing trend of data abundance and diversity, presenting opportunities to enhance transit system effectiveness but requiring specialized knowledge and experience with analytics for harnessing such data. Transportation agencies and companies overseeing other modes and emerging mobility services are faced with the same challenges. This course provides students with in-depth exposure to emerging data types, sources and standards for transit and other mobility systems . The course will cover a range of analytics for harnessing diverse data in a variety of  planning and management applications. While special focus will be given to transit applications, other mobility modes and services will be considered as appropriate.</p>"},{"location":"Courses/#learning-objectives","title":"Learning Objectives","text":"<p>By the end of this course, students will:</p> <ul> <li>gain in-depth knowledge in various transit and mobility data types, sources and standards;</li> <li>demonstrate competence in data analysis and modelling;</li> <li>apply appropriate analytical methods to different types of transit problems and selected mobility applications;</li> <li>gain hands-on experience in commercial and open-source tools; and</li> <li>communicate results in attractive visual and textual forms.</li> </ul>"},{"location":"Courses/#course-schedule","title":"Course Schedule","text":"Week # Lectures + Lab Tasks 1 Course introduction, transit data, building the toolbox,exploratory data analysis Assignment 1 released 2 Reliability analysis, origin-destination demand estimation Project start 3 Spatial analysis, accessibility and equity analysis Assignment 2 released 4 Advanced AI and time-series methods and applications to transit/mobility systems 5 Ridership analysis and modelling using advanced analytics Assignment 3 released 6 Ridership analysis and modelling using advanced analytics 7 Operational control using reinforcement learning 8 Course wrap-up Project presentations"},{"location":"Projects/projects/","title":"Projects","text":""},{"location":"Projects/projects/#on-demand-transit","title":"On-Demand Transit","text":""},{"location":"Projects/projects/#miway-and-university-of-torontocanada-2024","title":"MiWay and University of Toronto/Canada (2024)","text":"<p>Evaluating the application of an On-Demand Transit (ODT) planning and assessment framework in collaboration with MiWay Transit - Mississauga.</p>"},{"location":"Projects/projects/#icity-20","title":"iCity 2.0","text":""},{"location":"Projects/projects/#university-of-torontocanada-2024","title":"University of Toronto/Canada (2024)","text":"<p>Leading the research delivery on Generative Design of Integrated Transportation Networks in the iCity 2.0 Project. This initiative aims to bring together academia, government, and industry to collaborate on Urban Data Science for Future Mobility. Our objective is to develop a new capability that employs generative design to discover the most effective surface transit service and network solutions, and identify optimal strategies for improving them in an Ontario case study. The Principal Investigator is Dr. Eric J. Miller. Ontario Research Fund: $4M CAD.</p> <p>Link : https://uttri.utoronto.ca/news/icity-2-0-urban-data-science-for-future-mobility-receives-funding-from-the-ontario-research-fund-research-excellence-round-10/</p>"},{"location":"Projects/projects/#transit-network-redesign","title":"Transit Network Redesign","text":""},{"location":"Projects/projects/#fundacao-getulio-vargas-fgvbrazil-2024","title":"Funda\u00e7\u00e3o Get\u00falio Vargas (FGV)/Brazil (2024)","text":"<p>Supporting strategic technical decision-making in Intelligent Transport Systems and advanced data analysis. Responsibilities include updating and restructuring the metropolitan network modeling and public passenger transport systems for the Porto Alegre Metropolitan Region. A key goal is to develop a comprehensive bid for implementing these technological improvements. This $1M USD project is led by Principal Investigator Dr. Ciro Biderman under Contract SECON N\u00ba 22343/2023.</p>"},{"location":"Projects/projects/#automated-fare-collection-and-abt-system-design","title":"Automated Fare Collection and ABT System Design","text":""},{"location":"Projects/projects/#fundacao-getulio-vargas-fgvbrazil-2023","title":"Funda\u00e7\u00e3o Get\u00falio Vargas (FGV)/Brazil (2023)","text":"<p>Designed the technical and functional architecture for transitioning from a closed-loop to an open-loop payment system with Account-Based Ticketing (ABT) services for EPTC in Porto Alegre, Brazil. This $390K USD project, led by Principal Investigator Dr. Ciro Biderman under Contract SECON N\u00ba 82352/2023, aimed to modernize fare collection systems and enhance passenger experience.</p> <p>Link: https://dopaonlineupload.procempa.com.br/dopaonlineupload/4726_ce_20230328_executivo.pdf</p>"},{"location":"Projects/projects/#big-data-for-sustainable-urban-development","title":"Big Data for Sustainable Urban Development","text":""},{"location":"Projects/projects/#fundacao-getulio-vargas-fgvbrazil-2021","title":"Funda\u00e7\u00e3o Get\u00falio Vargas (FGV)/Brazil (2021)","text":"<p>Collaborated with the Getulio Vargas Foundation (FGV-Brazil), Inter-American Development Bank (IDB), and Waze to develop a city-to-street-level congestion indicator. Designed and implemented a real-time tra\ufb03c monitoring dashboard for S\u00e3o Paulo (Brazil), Montevideo (Uruguay), Quito (Ecuador), Xalapa (Mexico), and Mira\ufb02ores (Peru). This initiative was part of the \"Big Data for Sustainable Urban Development\" project, led by Principal Investigator Dr. Ciro Biderman, to enhance urban mobility and decision-making through advanced data analytics.</p> <p>Link: https://publications.iadb.org/publications/english/document/Big-Data-for-Sustainable-Urban-Developement.pdf</p>"},{"location":"Projects/projects/#transit-equity-dashboard","title":"Transit Equity Dashboard","text":""},{"location":"Projects/projects/#transitcenter-usa-2021","title":"TransitCenter/ USA (2021)","text":"<p>Contributed to a $100K USD project led by Prof. Alex Karner, focusing on comprehensive transit access and equity analysis across seven U.S. major urban areas. Led the technical development and created a Python-based Fare Calculator system, a key component of the transit access score calculation, to support equity-focused transit planning.</p> <p>Link: https://dashboard.transitcenter.org/</p>"},{"location":"blog/","title":"Transit Data Blog","text":""},{"location":"blog/#hello-transit-data-scientists","title":"Hello Transit Data Scientists!","text":"<p>Hello and welcome!</p> <p>This blog is my space to share insights, ideas, and discussions on Transit Data Analytics. Whether you're a researcher, a professional in the public transport sector, or simply someone curious about how data can improve urban mobility, I hope you'll find valuable content here.</p> <p>What to Expect</p> <p>I\u2019ll be using this platform to:</p> <p>\ud83d\ude86 Communicate Advances in Transit Data Analytics \u2013 Public transportation systems generate vast amounts of data, and leveraging this data effectively is key to improving service reliability, efficiency, and user experience. I\u2019ll share updates on the latest methodologies, tools, and findings that push the field forward.</p> <p>\ud83d\udcda Discuss Academic Papers \u2013 Research plays a crucial role in shaping transportation policies and innovations. I\u2019ll be reviewing and discussing key papers, breaking down complex concepts, and highlighting their practical implications for transit planning and operations.</p> <p>\ud83d\udee0\ufe0f Create Tutorials with Practical Tips \u2013 Data analysis is a powerful tool, but knowing how to apply it in real-world transit scenarios is essential. Expect hands-on tutorials covering data processing, visualization, modeling, and machine learning techniques tailored for public transport applications.</p> <p>Why This Matters</p> <p>Public transport is the backbone of urban mobility, yet challenges such as congestion, delays, and service reliability persist. With the right data-driven approaches, we can design smarter, more efficient transit networks that benefit both operators and passengers. Through this blog, I aim to contribute to that mission by sharing knowledge and fostering discussions that drive meaningful improvements.</p> <p>Let\u2019s Connect</p> <p>I encourage you to share your thoughts, ask questions, and contribute to the conversation. Whether through comments, messages, or collaborations, I\u2019d love to hear from you!</p> <p>Stay tuned for upcoming posts, and let\u2019s dive into the fascinating world of Transit Data Analytics together.</p> <p>\ud83d\ude80 Let\u2019s move transit forward\u2014one data point at a time!</p> <p></p>"},{"location":"blog/2025/02/13/getting-started-with-ai-agents/","title":"Getting Started with AI Agents - Part I","text":"<p>It's hard to have a conversation about AI these days without bringing up tools like ChatGPT, DeepSeek, and the like, right? These tools are becoming key players in how we analyze and interact with public transport. Are you ready for this shift?</p> <p>In this post, let's chat a bit about AI Agents and how we can tailor LLMs to answer the questions that matter most to us as Transit Data Scientists.</p> <p>Before diving into more complex applications, let\u2019s take a moment to introduce the topic. This way, we can get comfortable with the terms and concepts we'll be working with!</p> <p>This tutorial is a summmary of HuggingFace Course on Agents - Unit 1.</p>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#whats-an-ai-agent-anyway","title":"What\u2019s an AI Agent, Anyway?","text":"<p>An AI Agent is just a smart system that uses AI to interact with its surroundings and get stuff done. It thinks, plans, and takes action (sometimes using extra tools) to complete tasks.</p> <p>AI Agents can do anything we set them up for using Tools to carry out Actions.</p>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#the-two-main-parts-of-an-ai-agent","title":"The Two Main Parts of an AI Agent","text":"<ol> <li> <p>The Brain (AI Model) - This is where all the decision-making happens. The AI figures out what to do next. Examples include Large Language Models (LLMs) like GPT-4.</p> </li> <li> <p>The Body (Tools &amp; Capabilities) - This is what the agent actually does. Its abilities depend on the tools it has access to.</p> </li> </ol>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#why-use-llms","title":"Why Use LLMs?","text":"<p>LLMs (Large Language Models) are the go-to choice for AI Agents because they\u2019re great at understanding and generating text. Popular ones include GPT-4, Llama, and Gemini.</p> <p>There are two ways you can use an LLM:</p> <ul> <li>Run Locally (if your computer is powerful enough).</li> <li>Use a Cloud/API (e.g., via Hugging Face\u2019s API).</li> </ul>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#system-messages-setting-the-rules","title":"System Messages: Setting the Rules","text":"<p>System messages (or prompts) tell the AI how it should behave. They act as guiding instructions.</p> <pre><code>system_message = {\n    \"role\": \"system\",\n    \"content\": \"You are a helpful customer service agent. Always be polite and clear.\"\n}\n</code></pre> <p>These messages also define what tools the AI can use and how it should format its responses.</p>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#conversations-how-ai-talks-to-users","title":"Conversations: How AI Talks to Users","text":"<p>A conversation is just back-and-forth messages between a user and the AI. Chat templates help keep things organized and make sure the AI remembers what\u2019s going on.</p> <p>Example:</p> <pre><code>conversation = [\n    {\"role\": \"user\", \"content\": \"I need help with my order\"},\n    {\"role\": \"assistant\", \"content\": \"Sure! What\u2019s your order number?\"},\n    {\"role\": \"user\", \"content\": \"ORDER-123\"},\n]\n</code></pre>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#chat-templates-keeping-ai-conversations-structured","title":"Chat Templates: Keeping AI Conversations Structured","text":"<p>Chat templates make sure LLMs correctly process messages. There are two main types of AI models:</p> <ul> <li>Base Models: Trained on raw text to predict the next word.</li> <li>Instruct Models: Fine-tuned to follow instructions and have conversations.</li> </ul> <p>We use ChatML, a structured format for messages. The transformers library takes care of this automatically:</p> <pre><code>from transformers import AutoTokenizer\n\ntokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-1.7B-Instruct\")\nrendered_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n</code></pre>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#what-are-tools","title":"What are Tools?","text":"<p>Tools let AI Agents do more than just text generation. A Tool is basically a function the AI can call to get things done.</p> Tool What It Does Web Search Fetches up-to-date info from the internet. Image Generation Creates images from text. Retrieval Pulls in data from other sources. API Interface Connects with external APIs like GitHub or Spotify.","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#why-do-ai-agents-need-tools","title":"Why Do AI Agents Need Tools?","text":"<p>LLMs have a limited knowledge base (they only know what they were trained on). Tools help by allowing:</p> <ul> <li>Real-time data fetching (e.g., checking the weather).</li> <li>Specialized tasks (e.g., doing math, calling APIs).</li> </ul>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#building-a-simple-tool-a-calculator","title":"Building a Simple Tool: A Calculator","text":"<p>Let\u2019s create a basic calculator tool that multiplies two numbers:</p> <pre><code>def calculator(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two numbers.\"\"\"\n    return a * b\n</code></pre> <p>This tool includes:</p> <ul> <li>A clear name (<code>calculator</code>).</li> <li>A description (via the docstring).</li> <li>Input and output types.</li> </ul> <p>To define it as a tool, we describe it like this:</p> <pre><code>Tool Name: calculator, Description: Multiplies two numbers., Arguments: a: int, b: int, Outputs: int\n</code></pre>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#automating-tool-descriptions","title":"Automating Tool Descriptions","text":"<p>Instead of writing descriptions manually, we can use Python introspection to extract details automatically. The <code>Tool</code> class helps manage this info.</p> <pre><code>class Tool:\n    \"\"\"\n    A class representing a reusable piece of code (Tool).\n\n    Attributes:\n        name (str): Name of the tool.\n        description (str): A textual description of what the tool does.\n        func (callable): The function this tool wraps.\n        arguments (list): A list of argument.\n        outputs (str or list): The return type(s) of the wrapped function.\n    \"\"\"\n    def __init__(self, \n                 name: str, \n                 description: str, \n                 func: callable, \n                 arguments: list,\n                 outputs: str):\n        self.name = name\n        self.description = description\n        self.func = func\n        self.arguments = arguments\n        self.outputs = outputs\n\n    def to_string(self) -&gt; str:\n        \"\"\"\n        Return a string representation of the tool, \n        including its name, description, arguments, and outputs.\n        \"\"\"\n        args_str = \", \".join([\n            f\"{arg_name}: {arg_type}\" for arg_name, arg_type in self.arguments\n        ])\n\n        return (\n            f\"Tool Name: {self.name},\"\n            f\" Description: {self.description},\"\n            f\" Arguments: {args_str},\"\n            f\" Outputs: {self.outputs}\"\n        )\n\n    def __call__(self, *args, **kwargs):\n        \"\"\"\n        Invoke the underlying function (callable) with provided arguments.\n        \"\"\"\n        return self.func(*args, **kwargs)\n</code></pre> <p>Now, we can create a tool instance:</p> <pre><code>calculator_tool = Tool(\n    \"calculator\",                   # name\n    \"Multiply two integers.\",       # description\n    calculator,                     # function to call\n    [(\"a\", \"int\"), (\"b\", \"int\")],   # inputs (names and types)\n    \"int\",                          # output\n)\n</code></pre>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#using-a-decorator-to-define-tools","title":"Using a Decorator to Define Tools","text":"<p>A decorator makes tool creation easier:</p> <pre><code>def tool(func):\n    \"\"\"\n    A decorator that creates a Tool instance from the given function.\n    \"\"\"\n    # Get the function signature\n    signature = inspect.signature(func)\n\n    # Extract (param_name, param_annotation) pairs for inputs\n    arguments = []\n    for param in signature.parameters.values():\n        annotation_name = (\n            param.annotation.__name__ \n            if hasattr(param.annotation, '__name__') \n            else str(param.annotation)\n        )\n        arguments.append((param.name, annotation_name))\n\n    # Determine the return annotation\n    return_annotation = signature.return_annotation\n    if return_annotation is inspect._empty:\n        outputs = \"No return annotation\"\n    else:\n        outputs = (\n            return_annotation.__name__ \n            if hasattr(return_annotation, '__name__') \n            else str(return_annotation)\n        )\n\n    # Use the function's docstring as the description (default if None)\n    description = func.__doc__ or \"No description provided.\"\n\n    # The function name becomes the Tool name\n    name = func.__name__\n\n    # Return a new Tool instance\n    return Tool(\n        name=name, \n        description=description, \n        func=func, \n        arguments=arguments, \n        outputs=outputs\n    )\n</code></pre> <p>Now, we can define tools like this:</p> <pre><code>@tool\ndef calculator(a: int, b: int) -&gt; int:\n    \"\"\"Multiply two integers.\"\"\"\n    return a * b\n\nprint(calculator.to_string())\n</code></pre> <p>This makes it easy for AI Agents to recognize and use tools based on text input.</p>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/13/getting-started-with-ai-agents/#recap","title":"Recap","text":"<ul> <li>AI Agents use AI models to interact and make decisions.</li> <li>LLMs handle language understanding and text generation.</li> <li>System Messages define the agent\u2019s behavior.</li> <li>Tools extend an AI\u2019s capabilities beyond text generation.</li> <li>Chat Templates format conversations properly.</li> <li>Tools help AI Agents fetch real-time data and execute tasks.</li> </ul> <p>By combining all these pieces, you can build smart AI Agents that think, act, and assist like pros!</p> <p>In the next tutorial we will discuss the AI \u200b\u200bAgents workflow.</p>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/15/getting-started-with-ai-agents-part-ii/","title":"Getting Started with AI Agents - Part II","text":"<p>We learned how tools are provided to agents in the system prompt and how AI agents can reason, plan, and interact with their environment.</p> <p>Now, we'll examine the AI Agent Workflow, known as Thought-Action-Observation.</p> <p>This tutorial is a summmary of HuggingFace Course on Agents - Unit 1.</p>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/15/getting-started-with-ai-agents-part-ii/#the-thought-action-observation-cycle","title":"The Thought-Action-Observation Cycle","text":"<p>The Think, Act, and Observe components operate in a continuous loop, following the flow outlined below:</p> <p></p> <p>After performing an action, the framework follows these steps in order:</p> <ul> <li>Parse the action to identify the function(s) to call and the argument(s) to use.</li> <li>Execute the action.</li> <li>Append the result as an Observation.</li> </ul> <p>In many agent frameworks, rules and guidelines are directly embedded into the system prompt. In the following System Message example we defined we can see:</p> <ul> <li> <p>The Agent\u2019s behavior.</p> </li> <li> <p>The Tools our Agent has access to.</p> </li> <li> <p>The Thought-Action-Observation Cycle, that we insert into the LLM instructions.</p> </li> </ul> <p></p> <p>Let\u2019s consider a practical example. Imagine we ask an agent about the temperature in Toronto. When the agent receives this question, it begins the initial step of the \"Think\" process. This \"Think\" step represents the agent\u2019s internal reasoning and planning activities to solve the task at hand. The agent utilizes its LLM capabilities to analyze the information presented in its prompt. </p> <p>During this process, the agent can break down complex problems into smaller, more manageable steps, reflect on past experiences, and continuously adjust its plans based on new information. Key components of this thought process include planning, analysis, decision-making, problem-solving, memory integration, self-reflection, goal-setting, and prioritization. </p> <p>For LLMs that are fine-tuned for function-calling, the thought process is optional.</p>  The user needs current weather information for Toronto. I have access to a tool that fetches weather data. First, I need to call the weather API to get up-to-date details.  <p>This step shows the agent breaking the problem into steps: first, gathering the necessary data.</p> <p>Based on its reasoning and the fact that the Agent is aware of a <code>get_weather</code> tool, the Agent prepares a JSON-formatted command to call the weather API tool. For example, its first action could be:</p> <p><pre><code>   {\n     \"action\": \"get_weather\",\n     \"action_input\": {\n       \"location\": \"Toronto\"\n     }\n   }\n</code></pre> The \"Observation\" step refers to the environment's response to an API call or the raw data received. This observation is then added to the prompt as additional context. Before the Agent formats and presents the final answer to the user, it returns to the \"Think\" step to update its internal reasoning. If the observation indicates an error or incomplete data, the Agent may re-enter the cycle to correct its approach.</p> <p>The ability to call external tools, such as a weather API, empowers the Agent to access real-time data, which is a critical capability for any effective AI agent. Each cycle prompts the Agent to integrate new information (observations) into its reasoning (thought process), ensuring that the final outcome is accurate and well-informed. This illustrates the core principle of the ReAct cycle: the dynamic interplay of Thought, Action, and Observation that enables AI agents to tackle complex tasks with precision and efficiency. By mastering these principles, you can design agents that not only reason through their tasks but also leverage external tools to achieve their objectives, continuously refining their outputs based on environmental feedback.</p>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/15/getting-started-with-ai-agents-part-ii/#the-react-approach","title":"The ReAct Approach","text":"<p>Another technique is the ReAct approach, which combines \u201cReasoning\u201d (Think) with \u201cActing\u201d (Act). ReAct is a straightforward prompting method that adds step-by-step reasoning before allowing the LLM to interpret the next tokens. In fact, encouraging the model to think like this promotes the decoding process towards the next tokens that create a plan, rather than jumping to a final solution, as the model is prompted to break the problem down into smaller tasks. This enables the model to examine sub-steps more thoroughly, which generally results in fewer errors compared to attempting to produce the final solution all at once. For instance, DeepSeek, which have been fine-tuned to \"think before answering\". These models have been trained to always include specific thinking sections (enclosed between  and  special tokens). This is not just a prompting technique like ReAct, but a training method where the model learns to generate these sections after analyzing thousands of examples that show what we expect it to do.</p>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/15/getting-started-with-ai-agents-part-ii/#actions","title":"Actions","text":"<p>Actions refer to the specific steps that an AI agent undertakes to engage with its surroundings. Whether it involves searching the internet for information or managing a physical device, every action is a purposeful task performed by the agent. For instance, an agent that aids in customer service could obtain customer information, provide support articles, or escalate problems to a human representative.</p> <p>There are multiple types of Agents that take actions differently:</p> Type of Agent Description JSON Agent The Action to take is specified in JSON format. Code Agent The Agent writes a code block that is interpreted externally. Function-calling Agent It is a subcategory of the JSON Agent which has been fine-tuned to generate a new message for each action. <p>Actions can serve several purposes: </p> <ol> <li>Information Gathering (e.g., web searches, database queries).</li> <li>Tool Usage (e.g., API calls, calculations).</li> <li>Environment Interaction (e.g., controlling devices).</li> <li>Communication (e.g., engaging with users).</li> </ol> <p>An essential aspect of an agent is its ability to stop generating new tokens once an action is complete, applicable across all formats (JSON, code, function-calling). This prevents unintended output and ensures clarity. The LLM handles text to describe the desired action and its parameters.</p> <p>One approach to implementing actions is known as the stop and parse approach. This method ensures that output generation is structured, using formats like JSON or code. It aims to avoid producing unnecessary tokens and to call the appropriate tool to extract the required parameters.</p> <pre><code>Thought: I need to check the current weather.\nAction :\n{\n  \"action\": \"get_weather\",\n  \"action_input\": {\"location\": \"Toronto\"}\n}\n</code></pre> <p>Function-calling agents operate similarly by structuring each action so that a designated function is invoked with the correct arguments.</p> <p>An alternative Action approach is using Code Agents. Instead of outputting a simple JSON object, a Code Agent generates an executable code block\u2014typically in a high-level language like Python.</p> <p></p> <p>This approach offers several advantages:</p> <ul> <li> <p>Expressiveness: Code can naturally represent complex logic, including loops, conditionals, and nested functions, providing greater flexibility than JSON.</p> </li> <li> <p>Modularity and Reusability: Generated code can include functions and modules that are reusable across different actions or tasks.</p> </li> <li> <p>Enhanced Debuggability: With a well-defined programming syntax, code errors are often easier to detect and correct.</p> </li> <li> <p>Direct Integration: Code Agents can integrate directly with external libraries and APIs, enabling more complex operations such as data processing or real-time decision making.</p> </li> </ul> <p>For example, a Code Agent tasked with fetching the weather might generate the following Python snippet:</p> <p><pre><code># Code Agent Example: Retrieve Weather Information\ndef get_weather(city):\n    import requests\n    api_url = f\"https://api.weather.com/v1/location/{city}?apiKey=YOUR_API_KEY\"\n    response = requests.get(api_url)\n    if response.status_code == 200:\n        data = response.json()\n        return data.get(\"weather\", \"No weather information available\")\n    else:\n        return \"Error: Unable to fetch weather data.\"\n\n# Execute the function and prepare the final answer\nresult = get_weather(\"Toronto\")\nfinal_answer = f\"The current weather in Toronto is: {result}\"\nprint(final_answer)\n</code></pre> This method also follows the stop and parse approach by clearly delimiting the code block and signaling when execution is complete by printing the <code>final_answer</code>.</p>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/15/getting-started-with-ai-agents-part-ii/#observation","title":"Observation","text":"<p>Observations are how an Agent perceives the consequences of its actions. We ca understand as signals from the environment that guide the next cycle of thought.</p> <p>In the observation phase, the agent:</p> <ul> <li>Collects Feedback: Receives confirmation of action success.</li> <li>Appends Results: Updates its memory with new information.</li> <li>Adapts Strategy: Refines future actions based on updated context.</li> </ul> <p>This process of using feedback helps the agent stay on track with its goals. It allows the agent to learn and adjust continuously based on real-world results. Observation can also be seen as Tool \u201clogs\u201d that provide textual feedback of the Action execution.</p> <p>Type of Observation and Examples</p> <ol> <li>System Feedback: Error messages, success notifications, or status codes.</li> <li>Data Changes: Updates in the database, modifications to the file system, or changes in state.</li> <li>Environmental Data: Readings from sensors, system metrics, or resource usage information.</li> <li>Response Analysis: Responses from APIs, query results, or outputs from computations.</li> <li>Time-based Events: Completion of scheduled tasks or milestones reached, such as deadlines.</li> </ol>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/15/getting-started-with-ai-agents-part-ii/#comments","title":"Comments","text":"<p>Both tutorials may seem technical, but they offer an overview of understanding the potential of AI agents. In the next blog post, we will discuss an implementation in public transit.</p>","tags":["AI","agents","LLM","automation"]},{"location":"blog/2025/02/24/deploy-machine-learning-models-on-huggingface-hub-using-gradio/","title":"Deploy Machine Learning Models on HuggingFace Hub using Gradio?","text":"<p>In this tutorial, we will learn how to deploy models to Hugging Face Spaces. Hugging Face Spaces is a platform designed to create, share, and deploy machine learning (ML) demo applications, like an API (you can also call your app).</p> <p>Each Spaces environment is limited to 16 GB of RAM, 2 CPU cores, and 50 GB of non-persistent disk space by default, which is available free of charge. If you require more resources, you have the option to upgrade to better hardware.</p> <p>The first step is to create an account at the HuggingFace website.</p> <p></p> <p>Next, we will create a new space by clicking on the upper bar \"Spaces\" and the button +New Space. Next, we will create a new space by clicking on the upper bar and the button +New Space. We can select the \"Free\" version for the space hardware and set it up as a \"Public\" space.</p> <p>As you can see, we selected Gradio as our web tool.</p> <p></p> <p>This is the result after creating the space:</p> <p></p> <p>We still need to add two files to our project: <code>app.py</code> and <code>requirements.txt</code>.</p> <p>Let's create the requirements first:</p> <p></p> <p>After creating the \"requirements.txt\" file, the next step is to create an \"app.py\" file. To do this, we will copy the code below and paste it into a new file.</p> <p><pre><code>import gradio as gr\nfrom transformers import pipeline\n\n# load the image-text pipeline with Blip architecture\npipe = pipeline(\"image-to-text\",\n                model=\"Salesforce/blip-image-captioning-base\")\n\n# this function receives the input, call the pipeline\n# and get the generated text from the output\ndef launch(input):\n    out = pipe(input)\n    return out[0]['generated_text']\n\n# gradio interface, input is an image and output text\niface = gr.Interface(launch,\n                     inputs=gr.Image(type='pil'),\n                     outputs=\"text\")\n\n# if you want to share the link set \"share=True\"\niface.launch()\n</code></pre> Once you commit your files and click on the App tab, Space will automatically load the required files and interface. After you finish running the files, you will see the screen below, which serves as our interface for interacting with the app.</p> <p></p> <p>Time to test our solution!</p> <p> </p> <p>At the bottom of your app screen, you can click \"Use via API\" to see sample code that you can use to use your model with an API call.</p> <p></p> <p>To run the program locally, copy and paste the code snippet onto your machine and execute the file. Remember to specify the path to the input and to call the API using the endpoint \"/predict\".</p> <p>If you wish to keep your API private, you must pass your TOKEN to make the call.</p>","tags":["AI","agents","LLM","platforms","HuggingFace"]},{"location":"blog/2025/02/24/deploy-machine-learning-models-on-huggingface-hub-using-gradio/#references","title":"References","text":"<p>This tutorial is based on the course \"DeepLearning.AI-Open Source Models with Hugging Face.\"</p>","tags":["AI","agents","LLM","platforms","HuggingFace"]},{"location":"blog/archive/2025/","title":"2025","text":""},{"location":"blog/category/ai-platforms/","title":"AI Platforms","text":""},{"location":"blog/category/ai-agents/","title":"AI Agents","text":""}]}